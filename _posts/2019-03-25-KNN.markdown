---
layout:     post
title:      k-近邻算法
subtitle:   ""
date:       2019-03-25 10:14:00
author:     "Wp.Zhang"
header-img: "img/post-bg-2015.jpg"
tags:
    - Machine Learning
    - Data Science
---
## k-近邻算法基本实现

### 通用函数


```python
import numpy as np
import operator

def createDataSet():
    """
    创建数据集
    """
    group = np.array([[1,1.1], [1,1], [0,0], [0,0.1]], dtype='float')
    labels = ['A','A','B','B']
    return group, labels
```

测试


```python
group, labels = createDataSet()
group
labels
```




    array([[1. , 1.1],
           [1. , 1. ],
           [0. , 0. ],
           [0. , 0.1]])






    ['A', 'A', 'B', 'B']



该函数产生4个点，点的类别分别为A,A,B,B

### 算法实现

- classify0()函数的四个参数分别为：用于分类的输入向量input_X，训练样本集dataSet，标签向量labels，选择最近邻居的数目k。显然，labels的元素数目要和dataSet的行数相同（这样每一个样本才有所属的标签）。


- 距离计算使用欧式距离计算公式：<img src="http://latex.codecogs.com/gif.latex?d%20%3D%20%5Csqrt%7B%28xA_%7B0%7D%20-%20xB_%7B0%7D%29%5E%7B2%7D%20&plus;%20%28xA_%7B1%7D%20-%20xB_%7B1%7D%29%5E%7B2%7D%7D"></img>，拓展到多维数据距离的计算则公式为：$d>，拓展到多维数据距离的计算则公式为：<img src="http://latex.codecogs.com/gif.latex?%24d%20%3D%20%5Csqrt%7B%5Csum_%7Bi%7D%5E%7Bn%7D%7B%28xA_%7Bi%7D-xB_%7Bi%7D%29%5E%7B2%7D%7D%7D%24"></img> 其中n为维度。

##### 算法步骤

1. 首先计算所有点之间的距离；
2. 将计算结果从小到大依次排序；
3. 确定前k个距离最小的元素所在的主要分类；
4. 返回出现频率最高的元素标签。



```python
def classify0 (input_X, dataSet, labels, k):
    dataSetSize = dataSet.shape[0]  # 数据集列数
    # 计算距离
    diffMat = np.tile(input_X, (dataSetSize,1)) - dataSet
    # np.tile(input_X, (dataSetSize,1))将点input_X复制 数据集中所有点的个数 次，
    # 得出的结果减去dataSet后就求得了input_X与数据集中各点的坐标差
    sqDiffMat = diffMat**2  # 平方
    sqDistances = sqDiffMat.sum(axis=1)  # 求和
    distances = sqDistances**0.5  # 开方得出距离
    sortedDistIndicies = distances.argsort()  # argsort()将数组从小到大排序，返回每个数在原数组中的索引组成的列表
    
    # 选择距离最小的k个点
    classCount = {}  # 用于统计距离input_X最近的k个点的label分布，格式类似于{'A':1, 'B':2}
    for i in range(k):
        voteLabel = labels[sortedDistIndicies[i]]  # 获取距离input_X第i近的点的label
        classCount[voteLabel] = classCount.get(voteLabel, 0) + 1  # 该label出现次数+1
    
    # 排序
    sortedClassCount = sorted(classCount.items(), key=lambda x:x[1], reverse=True)  # 将label出现次数由大到小排序
    #sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True) # 原书上的代码
    return sortedClassCount[0][0]  # 返回出现次数最多的label
```


```python
classify0([0,0], group, labels, 3)
```




    'B'



## 利用k-近邻算法改进约会网站配对效果


```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False
```

### 数据读取

#### 文件格式处理

由于原始的数据文件是.txt类型文件，数据之间用'\t'分隔，于是将其读取处理后存为.csv文件便于数据操作

txt2csv函数将.txt文件转化为.csv文件
参数：fileName:文件名（文件路径），columns:数据所对应的列名组成的列表


```python
def txt2csv(fileName, columns):
    # 打开文件按行读取
    with open(fileName, 'r') as f:
        data = f.readlines()
    # 将每一行数据间隔方式由'\t'改为','
    for index, line in enumerate(data):
        data[index] = line.replace('\t', ',')
    # 将列名插入数据头部
    content = ','.join(columns) + '\n'
    content += ''.join(data)
    # 存储修改后的数据为.csv文件
    with open(fileName.split('.')[0]+'.csv', 'w', encoding='utf-8') as f:
        f.write(content)
```


```python
txt2csv('data/datingTestSet.txt', ['每年获得的飞行常客里程数','玩视频游戏所耗时间百分比','每周消费的冰淇淋公升数','评价'])
```


```python
df = pd.read_csv('data/datingTestSet.csv')
```


#### 文件读取

读取训练数据文件


```python
def splitData(fileName):
    data = pd.read_csv(fileName)
    columns = data.columns
    return data[columns[:-1]], data[columns[-1]]
```


```python
datingDataMat, datingLabels = splitData('data/datingTestSet.csv')
```

```python
df = pd.read_csv('data/datingTestSet.csv')
```

### 数据分析

分析“玩视频游戏所耗时间百分比”与“每周所消费的冰淇淋公升数”的关系


```python
sns.scatterplot(x="每年获得的飞行常客里程数", y="每周消费的冰淇淋公升数", hue="评价", data=df)
```

    <matplotlib.axes._subplots.AxesSubplot at 0x1e58fe9a400>

![](/media/editor/output_29_1_20190325181740236162.png)


观察各个特征之间的关系


```python
sns.pairplot(df, hue="评价", plot_kws={'alpha':0.3})
```

    <seaborn.axisgrid.PairGrid at 0x1e591194748>

![](/media/editor/output_31_2_20190325181753333190.png)

### 数据预处理

由于给出的三列特征取值范围不同，首先将它们的数值归一化


```python
def autoNorm(dataSet):
    minVals = dataSet.min()
    maxVals = dataSet.max()
    return (dataSet - minVals) / (maxVals - minVals)
```


```python
df['每年获得的飞行常客里程数'] = autoNorm(df['每年获得的飞行常客里程数'])
df['玩视频游戏所耗时间百分比'] = autoNorm(df['玩视频游戏所耗时间百分比'])
df['每周消费的冰淇淋公升数'] = autoNorm(df['每周消费的冰淇淋公升数'])
```


```python
df.head()
```

### 分类


```python
datingLabels[0]
```

定义一个包含了所有处理步骤的函数作为分类器


```python
def datingClassTest():
    # 文件处理
    txt2csv('data/datingTestSet.txt', ['每年获得的飞行常客里程数','玩视频游戏所耗时间百分比','每周消费的冰淇淋公升数','评价'])
    df = pd.read_csv('data/datingTestSet.csv')
    # 数据预处理-归一化
    df['每年获得的飞行常客里程数'] = autoNorm(df['每年获得的飞行常客里程数'])
    df['玩视频游戏所耗时间百分比'] = autoNorm(df['玩视频游戏所耗时间百分比'])
    df['每周消费的冰淇淋公升数'] = autoNorm(df['每周消费的冰淇淋公升数'])
    # 数据拆分
    datingDataMat, datingLabels = df[df.columns[:-1]], df[df.columns[-1]]
    # 测试
    test_ratio = 0.1  # 测试集比例
    test_length = int(df.shape[0]*test_ratio)  # 测试集大小
    error_count = 0  # 分类错误个数统计
    for i in range(test_length):
        # 这里之前用的是pandas.DataFrame数据类型，而分类函数中用的是numpy.array类型，所以要转化一下
        input_X = datingDataMat.iloc[[i]].values
        dataSet = datingDataMat.iloc[test_length:].values
        labels = datingLabels[test_length:].values
        classifierResult = classify0(input_X, dataSet, labels, 3)
        print("the classifier came back with:%s, the real answer is:%s" % (classifierResult, datingLabels[i]))
        if classifierResult != datingLabels[i]:
            error_count += 1
    print("the total error rate is : %f" % (error_count/test_length))
```


```python
datingClassTest()
```

可见错误率为0.05，正确率有95%，比较令人满意

如果要根据已有结果进行预测，测调用分类函数，将当前所有的数据作为dataSet参数即可

## 手写识别系统

### 准备数据

为了便于操作，需要将手写数字的图片都转化为0，1矩阵表示的文本

由于图片已经事先转换完成，于是只写读取txt文本中矩阵到numpy的函数

img2vector函数将读取的txt文件中格式为32*32的0-1矩阵读取到numpy数组中后，将其转化为一个长为1024的数组


```python
def img2vector(fileName):
    # 读取文件
    with open(fileName, 'r') as f:
        data = f.readlines()
    data = [_.replace('\n','') for _ in data]
    # 处理数据，将字符型转化为整型
    for i,lines in enumerate(data):
        lines = list(lines)
        for j, letter in enumerate(lines):
            lines[j] = int(letter)
        data[i] = lines
    # 将矩阵转化为数组
    data = np.array(data).reshape(1,1024)
    
    return data
```


```python
img2vector('data/digits/trainingDigits/0_0.txt')
```




    array([[0, 0, 0, ..., 0, 0, 0]])



### 手写数字识别系统


```python
import os
```


```python
def handwritingClassTest():
    labels = []  # 存储数据标签的列表
    trainingFileList = os.listdir('data/digits/trainingDigits')  # 获取所有训练数据文件列表
    hight = len(trainingFileList)  # 训练数据个数
    trainingMat = np.zeros((hight, 1024))  # 存储所有训练数据
    # 读取并存储训练数据
    for i in range(hight):
        fileName = trainingFileList[i]
        label = int(fileName.split('_')[0])  # 根据文件名获取标签
        labels.append(label)
        trainingMat[i, :] = img2vector('data/digits/trainingDigits/%s' % fileName)  # 将文件转存为矩阵
    
    # 测试数据
    error_count = 0  # 分类错误计数
    testFileList = os.listdir('data/digits/testDigits')  # 获取所有测试数据文件列表
    hight_test = len(testFileList)  # 测试数据个数
    # 读取并存储测试数据
    for i in range(hight_test):
        fileName = testFileList[i]
        label = int(fileName.split('_')[0])  # 根据文件名获取标签
        vector = img2vector('data/digits/trainingDigits/%s' % fileName)  # 将文件转存为矩阵
        classifierResult = classify0(vector, trainingMat, labels, 3)  # 使用k-近邻算法预测其类别
        print("the classifier came back with: %d, the real answer is: %d" % (classifierResult, label))
        if classifierResult!= label:
            error_count += 1
    
    print("\nthe total number of errors is: %d" % error_count)
    print("the total error rate is: %f" % (error_count/hight_test))
```


```python
handwritingClassTest()
```

    the classifier came back with: 0, the real answer is: 0
    the classifier came back with: 0, the real answer is: 0
    the classifier came back with: 0, the real answer is: 0
    the classifier came back with: 0, the real answer is: 0

    ...

    the classifier came back with: 9, the real answer is: 9
    the classifier came back with: 9, the real answer is: 9
    the classifier came back with: 9, the real answer is: 9
    the classifier came back with: 9, the real answer is: 9
    
    the total number of errors is: 13
    the total error rate is: 0.013742
    

错误总量为13个，错误率为0.013742，比较令人满意。