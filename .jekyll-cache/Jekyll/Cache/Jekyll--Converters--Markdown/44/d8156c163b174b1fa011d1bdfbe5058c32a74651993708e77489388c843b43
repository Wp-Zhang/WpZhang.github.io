I"(<h2 id="算法思想">算法思想</h2>

<p>        这个算法的思想就是不断地添加树，不断地进行特征分裂来生长一棵树，每次添加一棵树其实是学习一个新函数去拟合上次预测的残差（从这里就能看出XGB算法是基于GBDT算法的）。当训练完成得到k棵树，要预测一个样本的分数其实就是根据这个样本的特征，在每棵树中会落到对应的一个叶子节点，每个叶子节点对应一个分数，最后只需要将每棵树对应的分数加起来就是该样本的预测值。</p>

<h2 id="算法原理">算法原理</h2>

<p>        XGBoost目标函数定义为：</p>

<script type="math/tex; mode=display">Obj = \sum^n_{i=1}l(y_i,\hat{y}_i) + \sum^K_{k=1}Ω(f_k)\quad, \quad Ω(f) = \gamma T + \frac12\lambda\sum^T_{j=1}\omega^2_j</script>

<p>        目标函数由两部分构成，第一部分用来衡量预测分数和真实分数之间的差距，另一部分则是正则化项。正则化项也分为两部分，第一部分中T表示叶子节点个数，$\gamma$用于控制叶子节点的个数，另一部分中ω表示叶子节点的分数，λ可以控制叶子节点的分数不会过大，防止过拟合。</p>

<p>        之前说到过，新生成的树要拟合上次预测结果的残差，当生成t棵树后，预测分数可以写成:</p>

<script type="math/tex; mode=display">\hat{y}_i^{(t)} = \hat{y}_i^{(t-1)} + f_t(x_i)</script>

<p>        同时可以将目标函数写成：</p>

<script type="math/tex; mode=display">L^{(t)} = \sum^n_{i=1}l(y_i,\hat{y}_i^{(t-1)} + f_t(x_i)) + Ω(f_t)</script>

<p>        显然，接下来的目标就是去找一个$f_t$使得目标函数值最小。XGboost的想法是利用其在$f_t=0$处的泰勒二阶展开近似它。近似的目标函数为：</p>

<script type="math/tex; mode=display">L^{(t)} = \sum^n_{i=1}[l(y_i,\hat{y}^{(t-1)}) + g_if_t(x_i) + \frac12 h_if^2_t(x_i)] + Ω(f_t)</script>

<p>        其中$g_i$为一阶导数，$h_i$为二阶导数：</p>

<script type="math/tex; mode=display">g_i = \partial_{\hat{y}^{(t-1)}}\,l(y_i,\hat{y}^{(t-1)}),\quad h_i = \partial^2_{\hat{y}^{(t-1)}}\,l(y_i,\hat{y}^{(t-1)})</script>

<p>        由于前t-1棵树的预测分数与y的残差对目标函数优化不影响$\color{red}{???}$，可以直接去掉。简化目标函数为：</p>

<script type="math/tex; mode=display">L^{(t)} = \sum^n_{i=1}[g_if_t(x_i) + \frac12h_if^2_t(x_i)] + Ω(f_t)</script>

<p>        上式是将每个样本的损失函数值加起来，但是根据前面所述的XGBoost的算法原理可知，每个样本最终会落到一个叶子节点中，所以我们可以将同一个叶子节点的样本重组起来，过程如下：</p>

<script type="math/tex; mode=display">Obj^{(t)}  \approx \sum^n_{i=1}[g_if_t(x_i) + \frac12h_if^2_t(x_i)] + Ω(f_t)

\\  = \sum^n_{i=1}[g_i\omega_{q(x_i)} + \frac12h_i\omega^2_{q(x_i)}] + \gamma T + \lambda\frac12\sum^T_{j=1}\omega^2_j

\\= \sum^T_{j=1}[(\sum_{i\in I_j} g_i)\omega_j + \frac12(\sum_{i\in I_j}h_i + \lambda)\omega^2_j] + \gamma T</script>

<p>        其中，$f_t(x) = \omega_{q(x)}\,,\;\omega\in R^T\,,\;q:R^d → 1,2,…T$，ω为叶子节点的得分值，q(x)表示样本x对应的叶子节点，$\omega_{q(x)}$描述了一整棵树的模型，T为该树的叶子节点个数。</p>

<p>        通过上式的改写，就可以将目标函数改写成叶子节点分数ω的一个一元二次函数，使得求解变得简单。对$\omega_j$求偏导并使其导函数等于0则可以得出最优ω和目标函数公式：</p>

<script type="math/tex; mode=display">\omega^*_j = -\frac{G_j}{H_j + \lambda}\qquad Obj = -\frac12 \sum^T_{j=1} \frac{G^2_j}{H_j+\lambda} + \gamma T</script>

<p>        Obj代表了当指定一个树的结构的时候，在目标上面最多减少多少。</p>

<p>        对于树结构分数，XGBoost和GBDT基本相同，都是首先初始化为一个常数，GBDT是根据一阶导数、XGBoost是根据一阶导数和二阶导数迭代生成基学习器，相加更新学习器。</p>

<h2 id="分裂节点算法">分裂节点算法</h2>

<p>        在传统的算法中，寻找分裂节点时使用了暴力遍历的方法，但是当数据量大时，传统算法将占用大量内存。XGBoost算法采用了一种近似算法来近似分裂节点，从而减少了算法运行时间。</p>

<p>        这个算法根据特征的分布情况作出一个初步的筛选，然后就在筛选的结果中选出分割点，这种做法可以大大提高效率。有两种筛选的方式，一种是global，一种是local。在global方法中，在每次建树之前就做一次筛选，然后每次分割都要更新一下结果；而local方法则是在每次分割之后更新筛选结果。判断是否分割和选择分割点依据分裂后的目标函数值比单子叶子节点的目标函数的增益，同时为了限制树生长过深，还加了个阈值，只有当增益大于该阈值才进行分裂。</p>

<h2 id="缺失值处理">缺失值处理</h2>

<p>        XGBoost的一个特点就是能够处理缺失值，允许缺失值的存在。</p>

<p>        原始论文中关于缺失值的处理将其看与稀疏矩阵的处理看作一样。在寻找分裂节点的时候，不会对该特征为缺失的样本进行遍历统计，只在该列没有缺失值的样本上对应的特征值进行遍历，通过这个技巧来减少了为稀疏离散特征寻找分裂节点的时间开销。在逻辑实现上，为了保证完备性，会分别处理将缺失该特征值的样本分配到左叶子结点和右叶子结点的两种情形，计算增益后选择增益大的方向进行分裂即可。可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率。如果在训练中没有缺失值而在预测中出现缺失，那么会自动将缺失值的划分方向放到右子树。</p>

<h2 id="优缺点">优缺点</h2>

<h3 id="优点">优点</h3>

<ul>
  <li>
    <p>支持线性分类器；</p>
  </li>
  <li>
    <p>当样本存在缺失值时，能够自动选择分类方向</p>
  </li>
  <li>
    <p>支持列抽样，防止过拟合的同时减少计算量</p>
  </li>
  <li>
    <p>目标函数引入正则化项，控制了模型的复杂度，降低了模型的方差，防止模型过拟合</p>
  </li>
  <li>
    <p>每次迭代后为叶子节点分配学习速率，降低每棵树的权重，减少每棵树的影响，为之后的学习提供更多的空间</p>
  </li>
  <li>
    <p>支持并行</p>
  </li>
  <li>
    <p>高效的分裂节点选择方法</p>
  </li>
</ul>

<h3 id="缺点">缺点</h3>

<ul>
  <li>
    <p>采用预排序，在开始迭代前对节点的特征做预排序，遍历选择最优分割点，当数据量大时仍很耗时</p>
  </li>
  <li>
    <p>同时分裂同一层的叶子，从而进行多线程优化，不容易过拟合，但很多叶子节点的分裂增益较低，没必要进行跟进一步的分裂，带来了不必要的开销</p>
  </li>
</ul>

<h2 id="应用场景">应用场景</h2>

<p>可用于所有回归问题(线性/非线性)，适用面非常广。</p>

<h2 id="参数设置">参数设置</h2>

<ul>
  <li>
    <p>eta（默认0.3）：每一步的权重，和GBDT中的learning rate参数类似，通过减少该值可以提高模型的稳定性</p>
  </li>
  <li>
    <p>min_child_weight（默认1）：最小叶子节点样本权重和。当它的值较大时，可以避免模型学习到局部的特殊样本。但是如果这个值过高，会导致欠拟合。这个参数需要使用 CV 来调整。</p>
  </li>
  <li>
    <p>max_depth（默认6）：树的最大深度，防止过拟合。</p>
  </li>
  <li>
    <p>max_leaf_nodes：树上叶子节点数量的最大值。可以替代max_depth，如果定义了这个参数将会忽略max_depth参数</p>
  </li>
  <li>
    <p>gamma（默认0）：在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。Gamma 指定了节点分裂所需的最小损失函数下降值。 这个参数的值越大，算法越保守。</p>
  </li>
  <li>
    <p>max_delta_step（默认0）：每棵树权重改变的最大步长。各类别的样本十分不平衡时，它对逻辑回归是很有帮助的。</p>
  </li>
  <li>
    <p>subsample（默认1）：控制对于每棵树随机采样的比例。值设置的过小可能导致欠拟合。</p>
  </li>
  <li>
    <p>colsample_bytree（默认1）：每棵树随机采样的列数（即特征）的占比</p>
  </li>
  <li>
    <p>colsample_bylevel（默认1）：树的每一级的每一次分裂对列数的采样的占比</p>
  </li>
  <li>
    <p>lambda（默认1）：权重的 L2 正则化项。控制XGBoost的正则化部分</p>
  </li>
  <li>
    <p>alpha（默认1）：权重的 L1 正则化项。可以应用在很高维度的情况下，使得算法的速度更快。</p>
  </li>
  <li>
    <p>scale_pos_weight（默认1）：在各类别样本十分不平衡时，把这个参数设定为一个正值，可以使算法更快收敛。</p>
  </li>
</ul>

<hr />

<ul>
  <li>
    <p>objective（默认reg:linear）：定义最小化的损失函数</p>

    <p>常用值：</p>

    <ul>
      <li>
        <p>binary:logistic二分类逻辑回归，返回预侧概率</p>
      </li>
      <li>
        <p>multi:softmax使用softmax的多分类器，返回预测的类别。在这种情况下，需要额外设置参数num_class（类别数目）</p>
      </li>
      <li>
        <p>multi:softprob和softmax相同，但是返回每个数据属于各个类别的概率</p>
      </li>
    </ul>
  </li>
  <li>
    <p>eval_metric：对于回归问题，默认rmse；对于分类问题，默认error</p>

    <p>常用值：</p>

    <ul>
      <li>
        <p>rmse均方根误差</p>
      </li>
      <li>
        <p>mae平均绝对误差</p>
      </li>
      <li>
        <p>logloss负对数似然函数值</p>
      </li>
      <li>
        <p>error二分类错误率</p>
      </li>
      <li>
        <p>merror多分类错误率</p>
      </li>
      <li>
        <p>mlogloss多分类logloss损失函数</p>
      </li>
      <li>
        <p>auc曲线下面积</p>
      </li>
    </ul>
  </li>
  <li>
    <p>seed（默认0）：可以复现随机数据的结果，也可以用于调整参数。</p>
  </li>
</ul>
:ET